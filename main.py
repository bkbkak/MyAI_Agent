import os
import argparse
import shutil
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import uuid

# --- é…ç½®éƒ¨åˆ† ---
# è¿™é‡Œè®¾ç½®æ•°æ®å­˜å‚¨åœ¨å½“å‰ç›®å½•ä¸‹çš„ db æ–‡ä»¶å¤¹
DB_PATH = os.path.join(os.getcwd(), "db")
PAPER_DIR = os.path.join(os.getcwd(), "library", "papers")
IMAGE_DIR = os.path.join(os.getcwd(), "library", "images")

# --- æ ¸å¿ƒåŠŸèƒ½ç±» ---
class AIAgent:
    def __init__(self):
        print("æ­£åœ¨åˆå§‹åŒ– AI æ¨¡å‹ (é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…)...")
        # 1. åŠ è½½æ–‡æœ¬æ¨¡å‹ (å¤„ç†è®ºæ–‡)
        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
        
        # 2. åŠ è½½å›¾åƒæ¨¡å‹ (å¤„ç†å›¾ç‰‡)
        self.clip_model = SentenceTransformer('clip-ViT-B-32', device='cpu')
        
        # 3. åˆå§‹åŒ–æ•°æ®åº“
        self.client = chromadb.PersistentClient(path=DB_PATH)
        self.paper_collection = self.client.get_or_create_collection("papers")
        self.image_collection = self.client.get_or_create_collection("images")
        
        # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
        os.makedirs(PAPER_DIR, exist_ok=True)
        os.makedirs(IMAGE_DIR, exist_ok=True)
        print("åˆå§‹åŒ–å®Œæˆï¼")

    def add_paper(self, file_path, topics=None):
        """æ·»åŠ è®ºæ–‡å¹¶è‡ªåŠ¨åˆ†ç±»"""
        if not os.path.exists(file_path):
            print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
            return

        print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
        # è¯»å– PDF æ–‡å­—
        try:
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages[:5]: # åªè¯»å‰5é¡µï¼ŒåŠ å¿«é€Ÿåº¦
                text += page.extract_text() + "\n"
        except Exception as e:
            print(f"è¯»å– PDF å¤±è´¥: {e}")
            return

        # ç®€å•çš„è‡ªåŠ¨åˆ†ç±»é€»è¾‘
        final_dir = PAPER_DIR
        if topics:
            topic_list = topics.split(',')
            # è®¡ç®—æ–‡æœ¬å’Œæ‰€æœ‰ä¸»é¢˜çš„ç›¸ä¼¼åº¦
            doc_emb = self.text_model.encode(text)
            topic_embs = self.text_model.encode(topic_list)
            
            from sentence_transformers import util
            scores = util.cos_sim(doc_emb, topic_embs)[0]
            best_topic = topic_list[scores.argmax()]
            
            final_dir = os.path.join(PAPER_DIR, best_topic)
            os.makedirs(final_dir, exist_ok=True)
            print(f"è‡ªåŠ¨å½’ç±»åˆ°: {best_topic}")

        # ç§»åŠ¨æ–‡ä»¶
        filename = os.path.basename(file_path)
        new_path = os.path.join(final_dir, filename)
        shutil.copy(file_path, new_path) # å¤åˆ¶æ–‡ä»¶è¿‡å»

        # å­˜å…¥æ•°æ®åº“
        embedding = self.text_model.encode(text).tolist()
        self.paper_collection.add(
            documents=[text[:500]], # åªå­˜å¼€å¤´éƒ¨åˆ†é¢„è§ˆ
            embeddings=[embedding],
            metadatas=[{"path": new_path, "filename": filename}],
            ids=[str(uuid.uuid4())]
        )
        print(f"âœ… è®ºæ–‡ '{filename}' å·²æˆåŠŸå½•å…¥ç³»ç»Ÿï¼")

    def search_paper(self, query):
        """æœç´¢è®ºæ–‡"""
        print(f"ğŸ” æ­£åœ¨æœç´¢: {query} ...")
        query_emb = self.text_model.encode(query).tolist()
        results = self.paper_collection.query(query_embeddings=[query_emb], n_results=3)
        
        for i, meta in enumerate(results['metadatas'][0]):
            print(f"[{i+1}] {meta['filename']}")
            print(f"    è·¯å¾„: {meta['path']}")

    def index_images(self, folder_path):
        """æ‰«æå¹¶ç´¢å¼•æ–‡ä»¶å¤¹é‡Œçš„å›¾ç‰‡"""
        from PIL import Image
        print(f"æ­£åœ¨æ‰«æå›¾ç‰‡æ–‡ä»¶å¤¹: {folder_path}")
        count = 0
        for root, _, files in os.walk(folder_path):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    path = os.path.join(root, file)
                    try:
                        img = Image.open(path)
                        emb = self.clip_model.encode(img).tolist()
                        self.image_collection.add(
                            embeddings=[emb],
                            metadatas=[{"path": path}],
                            ids=[str(uuid.uuid4())]
                        )
                        count += 1
                        print(f"å·²ç´¢å¼•: {file}")
                    except:
                        pass
        print(f"âœ… å®Œæˆï¼å…±ç´¢å¼• {count} å¼ å›¾ç‰‡ã€‚")

    def search_image(self, query):
        """ä»¥æ–‡æœå›¾"""
        print(f"ğŸ–¼ï¸ æ­£åœ¨å¯»æ‰¾å›¾ç‰‡: {query} ...")
        # CLIP æ¨¡å‹çš„ç‰¹æ®Šä¹‹å¤„ï¼šç”¨æ–‡æœ¬ç¼–ç å™¨æœå›¾ç‰‡åµŒå…¥
        query_emb = self.clip_model.encode(query).tolist()
        results = self.image_collection.query(query_embeddings=[query_emb], n_results=3)
        
        for i, meta in enumerate(results['metadatas'][0]):
            print(f"[{i+1}] è·¯å¾„: {meta['path']}")

# --- å‘½ä»¤è¡Œå…¥å£ ---
def main():
    parser = argparse.ArgumentParser(description="æˆ‘çš„ AI åŠ©æ‰‹")
    subparsers = parser.add_subparsers(dest='command')

    # å‘½ä»¤1: æ·»åŠ è®ºæ–‡
    p_add = subparsers.add_parser('add_paper')
    p_add.add_argument('path')
    p_add.add_argument('--topics')

    # å‘½ä»¤2: æœç´¢è®ºæ–‡
    p_search = subparsers.add_parser('search_paper')
    p_search.add_argument('query')

    # å‘½ä»¤3: ç´¢å¼•å›¾ç‰‡
    p_idx = subparsers.add_parser('index_images')
    p_idx.add_argument('path')

    # å‘½ä»¤4: æœå›¾
    p_img = subparsers.add_parser('search_image')
    p_img.add_argument('query')

    args = parser.parse_args()
    
    if args.command:
        agent = AIAgent()
        if args.command == 'add_paper':
            agent.add_paper(args.path, args.topics)
        elif args.command == 'search_paper':
            agent.search_paper(args.query)
        elif args.command == 'index_images':
            agent.index_images(args.path)
        elif args.command == 'search_image':
            agent.search_image(args.query)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
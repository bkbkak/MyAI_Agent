import os
import argparse
import shutil
import uuid
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer, util
import chromadb
from PIL import Image

# --- å…¨å±€è·¯å¾„é…ç½® ---
BASE_DIR = os.getcwd()
DB_PATH = os.path.join(BASE_DIR, "db")
PAPER_DIR = os.path.join(BASE_DIR, "library", "papers")
IMAGE_DIR = os.path.join(BASE_DIR, "library", "images")

class LocalAIAgent:
    def __init__(self):
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– AI æ¨¡å‹ ...")
        # 1. åŠ è½½æ¨¡å‹ (å¼ºåˆ¶ä½¿ç”¨ CPU)
        self.text_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
        self.clip_model = SentenceTransformer('clip-ViT-B-32', device='cpu')
        
        # 2. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.client = chromadb.PersistentClient(path=DB_PATH)
        self.paper_collection = self.client.get_or_create_collection("papers")
        self.image_collection = self.client.get_or_create_collection("images")
        
        # 3. ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(PAPER_DIR, exist_ok=True)
        os.makedirs(IMAGE_DIR, exist_ok=True)
        print("âœ… ç³»ç»Ÿå°±ç»ª\n" + "="*30)

    def _extract_text_from_pdf(self, file_path):
        """æå– PDF æ–‡æœ¬"""
        try:
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages[:5]:
                text += page.extract_text() + "\n"
            return text.strip() if text.strip() else None
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å– PDF {file_path}: {e}")
            return None

    # ================= æ ¸å¿ƒåŠŸèƒ½ï¼šæ–‡çŒ®ç®¡ç† =================

    def add_paper(self, file_path, topics=None):
        """æ·»åŠ è®ºæ–‡ï¼šåˆ†ç±» + ç‰©ç†ç§»åŠ¨ + ç´¢å¼•å»é‡"""
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

        text = self._extract_text_from_pdf(file_path)
        if not text:
            print(f"âš ï¸ è·³è¿‡ç©ºæ–‡ä»¶: {file_path}")
            return

        # 1. è¯­ä¹‰åˆ†ç±»
        target_dir = PAPER_DIR
        if topics:
            topic_list = [t.strip() for t in topics.split(',')]
            doc_emb = self.text_model.encode(text)
            topic_embs = self.text_model.encode(topic_list)
            scores = util.cos_sim(doc_emb, topic_embs)[0]
            best_topic = topic_list[scores.argmax()]
            target_dir = os.path.join(PAPER_DIR, best_topic)
            os.makedirs(target_dir, exist_ok=True)

        # 2. ç‰©ç†æ•´ç†
        filename = os.path.basename(file_path)
        new_path = os.path.join(target_dir, filename)
        if os.path.abspath(file_path) != os.path.abspath(new_path):
            shutil.copy(file_path, new_path)

        # 3. å»ºç«‹ç´¢å¼• (Upsert å®ç°å»é‡)
        # é€»è¾‘ï¼šå¦‚æœ ID (æ–‡ä»¶å) å·²å­˜åœ¨ï¼Œåˆ™æ›´æ–°ï¼›ä¸å­˜åœ¨åˆ™æ’å…¥ã€‚
        embedding = self.text_model.encode(text).tolist()
        self.paper_collection.upsert(
            ids=[filename], 
            embeddings=[embedding],
            metadatas=[{"path": new_path, "filename": filename}],
            documents=[text[:500]]
        )
        print(f"âœ… [å»é‡å¯¼å…¥] å·²å½’æ¡£: {filename} -> {os.path.basename(target_dir)}")

    def batch_organize(self, source_folder, topics):
        """æ‰¹é‡æ•´ç†"""
        print(f"ğŸ“‚ æ‰«ææ–‡ä»¶å¤¹: {source_folder}")
        files = [f for f in os.listdir(source_folder) if f.lower().endswith('.pdf')]
        if not files:
            print("âŒ æœªå‘ç° PDFã€‚")
            return
        for f in files:
            self.add_paper(os.path.join(source_folder, f), topics)
        print("âœ¨ æ‰¹é‡æ•´ç†å®Œæˆï¼")

    def search_paper(self, query):
        """ã€è‡ªé€‚åº”ç‰ˆã€‘æœè®ºæ–‡ï¼šæ™ºèƒ½è¿‡æ»¤æ— å…³ç»“æœ"""
        print(f"ğŸ” æœæ–‡çŒ®: '{query}'")
        query_emb = self.text_model.encode(query).tolist()
        
        results = self.paper_collection.query(
            query_embeddings=[query_emb],
            n_results=3,
            include=["metadatas", "distances"]
        )

        if not results['distances'][0]:
            print("âŒ æ–‡çŒ®åº“ä¸ºç©ºã€‚")
            return

        # --- åŠ¨æ€é˜ˆå€¼é€»è¾‘ (Adaptive Threshold) ---
        # æ–‡æœ¬åµŒå…¥é€šå¸¸æ¯”è¾ƒç´§å¯† (0~2ä¹‹é—´)ï¼Œæ‰€ä»¥é˜ˆå€¼å®¹å¿åº¦è®¾å°ä¸€ç‚¹
        best_score = results['distances'][0][0]
        # ç­–ç•¥ï¼šå…è®¸æ¯”ç¬¬ä¸€åå·® 0.5 (è·ç¦») ä»¥å†…çš„ç»“æœ
        # å¦‚æœç¬¬ä¸€åæ˜¯ 0.8ï¼Œé‚£ä¹ˆ 1.3 ä»¥å†…çš„æ‰æ˜¾ç¤ºï¼Œè¶…è¿‡ 1.3 çš„è¯´æ˜å·®è·å¤ªå¤§
        dynamic_threshold = best_score + 0.5 

        found = False
        print(f"(æœ€ä½³åŒ¹é…åˆ†: {best_score:.4f} | æ™ºèƒ½è¿‡æ»¤çº¿: {dynamic_threshold:.4f})")

        for i, (meta, dist) in enumerate(zip(results['metadatas'][0], results['distances'][0])):
            if dist <= dynamic_threshold:
                print(f"[{i+1}] âœ… åŒ¹é…: {meta['filename']} (åˆ†å€¼: {dist:.4f})")
                print(f"    ğŸ“ {meta['path']}")
                found = True
            else:
                # è¿‡æ»¤æ‰çš„ç»“æœï¼ˆå¯é€‰ï¼šæ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œä»¥å®Œå…¨éšè—ï¼‰
                pass # print(f"   [å·²è¿‡æ»¤] {meta['filename']} ç›¸å…³åº¦å¤ªä½ã€‚")

        if not found:
            print("âŒ æœªæ‰¾åˆ°é«˜åº¦ç›¸å…³çš„æ–‡çŒ®ã€‚")

    # ================= æ ¸å¿ƒåŠŸèƒ½ï¼šå›¾åƒç®¡ç† =================

    def index_images(self, folder_path):
        """å»ºç«‹å›¾ç‰‡ç´¢å¼• (Upsert å»é‡)"""
        print(f"ğŸ–¼ï¸ æ­£åœ¨ç´¢å¼•å›¾ç‰‡: {folder_path}")
        count = 0
        for root, _, files in os.walk(folder_path):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    path = os.path.join(root, file)
                    try:
                        img = Image.open(path)
                        emb = self.clip_model.encode(img).tolist()
                        # Upsert å®ç°å»é‡
                        self.image_collection.upsert(
                            ids=[file], 
                            embeddings=[emb],
                            metadatas=[{"path": path}]
                        )
                        count += 1
                    except:
                        pass
        print(f"âœ… [å»é‡å¯¼å…¥] å®Œæˆï¼å½“å‰å…±å¤„ç† {count} å¼ å›¾ç‰‡ã€‚")

    def search_image(self, query):
        """ã€è‡ªé€‚åº”ç‰ˆã€‘æœå›¾ç‰‡ï¼šæ™ºèƒ½è¿‡æ»¤æ— å…³ç»“æœ"""
        print(f"ğŸ–¼ï¸ æœå›¾: '{query}'")
        query_emb = self.clip_model.encode(query).tolist()
        
        results = self.image_collection.query(
            query_embeddings=[query_emb],
            n_results=3,
            include=["metadatas", "distances"]
        )
        
        if not results['distances'][0]:
            print("âŒ å›¾ç‰‡åº“ä¸ºç©ºã€‚")
            return

        # --- åŠ¨æ€é˜ˆå€¼é€»è¾‘ ---
        # CLIP çš„è·ç¦»åˆ†å€¼è¾ƒå¤§ (é€šå¸¸ 150~200)ï¼Œæ‰€ä»¥å®¹å¿åº¦ç»™å¤§ä¸€ç‚¹
        best_score = results['distances'][0][0]
        dynamic_threshold = best_score + 10.0 

        found = False
        print(f"(æœ€ä½³åŒ¹é…åˆ†: {best_score:.2f} | æ™ºèƒ½è¿‡æ»¤çº¿: {dynamic_threshold:.2f})")

        for i, (meta, dist) in enumerate(zip(results['metadatas'][0], results['distances'][0])):
            if dist <= dynamic_threshold:
                print(f"[{i+1}] âœ… åŒ¹é…: {meta['path']} (åˆ†å€¼: {dist:.4f})")
                found = True
            
        if not found:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…å›¾ç‰‡ã€‚")

# ================= å‘½ä»¤è¡Œå…¥å£ =================

def main():
    parser = argparse.ArgumentParser(description="Local AI Agent")
    subparsers = parser.add_subparsers(dest='command')

    p_add = subparsers.add_parser('add_paper')
    p_add.add_argument('path')
    p_add.add_argument('--topics')

    p_batch = subparsers.add_parser('batch_organize')
    p_batch.add_argument('folder')
    p_batch.add_argument('--topics')

    p_search = subparsers.add_parser('search_paper')
    p_search.add_argument('query')

    p_idx = subparsers.add_parser('index_images')
    p_idx.add_argument('path')

    p_img = subparsers.add_parser('search_image')
    p_img.add_argument('query')

    args = parser.parse_args()
    
    if args.command:
        agent = LocalAIAgent()
        if args.command == 'add_paper':
            agent.add_paper(args.path, args.topics)
        elif args.command == 'batch_organize':
            agent.batch_organize(args.folder, args.topics)
        elif args.command == 'search_paper':
            agent.search_paper(args.query)
        elif args.command == 'index_images':
            agent.index_images(args.path)
        elif args.command == 'search_image':
            agent.search_image(args.query)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
